---
title: 'Bonus: Adam Optimizer'

---



## ğŸ§  What is the Adam Optimizer?

Adam stands for:  
**Adaptive Moment Estimation**  

It's one of the most popular **optimization algorithms** used in training deep learning models.  
Adam is like a smart and upgraded version of **Stochastic Gradient Descent (SGD)**.  
It **adjusts the learning rate** for each parameter **automatically** and **uses past gradients** to make training faster and more stable.

---

## ğŸ§© How Does It Work?

Adam keeps track of two things during training:

1. **The average of gradients** â†’ called the **first moment** (like momentum).
2. **The average of squared gradients** â†’ called the **second moment** (like the variance).

> These are used to make better updates to weights.

---
## ğŸ”¢ Letâ€™s Start with â€œMomentsâ€ in Math

In statistics, the **moment** of a distribution is a way to describe its **shape**.

| Moment Type     | Formula (for a variable x)         | Meaning                         |
|------------------|-------------------------------------|----------------------------------|
| **1st moment**   | `E[x]`                              | Mean (average)                  |
| **2nd moment**   | `E[xÂ²]`                             | Raw variance (if centered: true variance) |
| **k-th moment**  | `E[x^k]`                            | Higher order shape descriptors |

So, in Adam:
- The **1st moment** of the gradients â†’ `E[gâ‚œ]` â†’ gives you an average of the gradients â†’ this is called **momentum** in optimization.
- The **2nd moment** of the gradients â†’ `E[gâ‚œÂ²]` â†’ gives you an average of the *squares* of gradients â†’ which relates to **variance**.

So they are using terminology from **statistics** and not just optimization.

---

## ğŸ” Now... Why Not Just Say "Momentum and Variance"?

Because:
- Adam is based on **statistical moments**, not exactly the same as classical "momentum" in physics.
- In classical **Momentum optimization**, you donâ€™t use bias correction or square the gradient.
- In classical **variance**, you subtract the mean â€” Adam doesnâ€™t do that either (it uses raw second moment).

> So to keep it general and precise, researchers use the term:
> - "1st moment" instead of â€œmomentumâ€  
> - "2nd moment" instead of â€œvarianceâ€

Itâ€™s a way to say:  
ğŸ§  â€œThis is *like* momentum and *like* variance â€” but not exactly.â€

---

## ğŸ§® Look at the Equations to Understand the Link

### ğŸ¥‡ First Moment (Mean of gradients â†’ like momentum):

$mâ‚œ = Î²â‚ * mâ‚œâ‚‹â‚ + (1 - Î²â‚) * gâ‚œ$

This is an **Exponential Moving Average (EMA)** of gradients â†’ it's smoothing the current gradient by mixing it with the past gradients.

This is exactly what **momentum** in optimization does â€” it remembers the direction youâ€™ve been heading and builds inertia.

âœ… Why call it momentum?
- It â€œcarries the gradient forwardâ€.
- Helps accelerate training in consistent directions.

---

### ğŸ¥ˆ Second Moment (Mean of squared gradients â†’ like variance):

$vâ‚œ = Î²â‚‚ * vâ‚œâ‚‹â‚ + (1 - Î²â‚‚) * gâ‚œÂ²$

This is the **EMA of the squared gradients**.

It captures how **large** or **bumpy** the gradients are â€” without considering direction (since itâ€™s squared).

This acts like a **proxy for variance**, because:
- Large gradient â†’ big $vâ‚œ$ â†’ maybe unstable â†’ slow down updates.
- Small gradient â†’ small $vâ‚œ$ â†’ stable â†’ take normal-sized step.

âœ… Why call it variance?
- It tells you the **spread** or **volatility** of the gradients.
- Used to **scale the update** inversely â€” large variance â†’ smaller step.


---

## ğŸ”¢ The Formula and Terms

Letâ€™s go step-by-step:

- Letâ€™s say:
  - $gâ‚œ$ = current gradient at time step $t$
  - $mâ‚œ$ = moving average of gradients (1st moment)
  - $vâ‚œ$ = moving average of squared gradients (2nd moment)
  - $Î¸â‚œ$ = parameters (weights) at time step $t$
  - $Î±`$= learning rate (e.g., 0.001)
  - $Î²â‚$ = decay rate for the 1st moment (commonly 0.9)
  - $Î²â‚‚$ = decay rate for the 2nd moment (commonly 0.999)
  - $Îµ`$= small value to avoid division by zero (e.g., 1e-8)

---

## ğŸ§® Step-by-Step Calculation

### 1. **Initialize**
$mâ‚€ = 0$
$vâ‚€ = 0$
$t = 0$


### 2. **At every time step t**:
Increment time:  
$t = t + 1$

#### â¤ Update the **1st moment (mâ‚œ):**
$mâ‚œ = Î²â‚ * mâ‚œâ‚‹â‚ + (1 - Î²â‚) * gâ‚œ$
ğŸ‘‰ This is the exponential moving average of gradients (like momentum).  
ğŸ‘‰ **Î²â‚ is the decay rate** â€” controls how much we remember past gradients.

#### â¤ Update the **2nd moment (vâ‚œ):**
$vâ‚œ = Î²â‚‚ * vâ‚œâ‚‹â‚ + (1 - Î²â‚‚) * (gâ‚œ)^2$
ğŸ‘‰ This is the exponential moving average of squared gradients.  
ğŸ‘‰ **Î²â‚‚ is the decay rate** â€” controls how much we remember past squared gradients.

> A **decay rate** (like Î²â‚=0.9) means:
> - Keep 90% of the previous value.
> - Add 10% of the new value.
> So the past has **more influence** but we slowly mix in new info.

#### â¤ **Bias Correction** (important for early steps):
$mÌ‚â‚œ = mâ‚œ / (1 - Î²â‚áµ—)$
$vÌ‚â‚œ = vâ‚œ / (1 - Î²â‚‚áµ—)$
ğŸ‘‰ These fix the fact that $mâ‚œ$ and $vâ‚œ$ start at 0 and are biased early on.

#### â¤ **Update Parameters (weights):**
$Î¸â‚œ = Î¸â‚œâ‚‹â‚ - Î± * mÌ‚â‚œ / (âˆšvÌ‚â‚œ + Îµ)$

This is the actual update.  
- If $vÌ‚â‚œ$ is large (i.e. the gradient is very bumpy), the step is **smaller**.  
- If $mÌ‚â‚œ$ is large and consistent, the step is **bigger**.  
- This is why Adam adapts smartly.

---

## ğŸ” Summary of Hyperparameters

| Symbol  | Meaning                        | Typical Value |
|---------|--------------------------------|---------------|
| Î±       | Learning rate                  | 0.001         |
| Î²â‚      | Decay rate for 1st moment      | 0.9           |
| Î²â‚‚      | Decay rate for 2nd moment      | 0.999         |
| Îµ       | Numerical stability constant   | 1e-8          |

---

## âœ… Why Use Adam?

- **Faster convergence** â€“ learns faster than plain SGD.
- **Stable** â€“ works well even with noisy gradients.
- **Adaptive** â€“ handles different parameter types automatically.

---

## ğŸ§  Real-Life Analogy

Imagine you're hiking down a mountain (minimizing the loss function):

- **Gradient** tells you the slope at each step.
- **Momentum (m)** remembers the direction of your past steps â†’ helps you speed up.
- **Variance (v)** tells you how bumpy or smooth the path is â†’ helps you slow down in shaky areas.
- **Adam** combines both â€” it remembers your past and adjusts your step size accordingly.

---



