{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ae33093-7a76-4d14-9cb0-161fcf982b03",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "In this notebook we will implement a logistic regression model that performs the functionality of a NAND gate. We will use the sigmoid function as the activation function and the cross-entropy loss function as the loss function. We will use the gradient descent algorithm to optimize the model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bd8a3e-3bda-47c8-980d-5dcfbe240ef3",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "Our data will have only 4 points, which are the 4 possible combinations of NAND gate inputs. The data is as follows:\n",
    "\n",
    "| x1  | x2  | y   |\n",
    "|-----|-----|-----|\n",
    "| 0   | 0   | 1   |\n",
    "| 0   | 1   | 1   |\n",
    "| 1   | 0   | 1   |\n",
    "| 1   | 1   | 0   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3f6732-437c-4b2b-9498-c8f25a90354f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the data points\n",
    "plt.scatter(x=[1],y=[1])\n",
    "plt.scatter(x=[0,0,1],y=[0,1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fa8478-318f-4542-ad28-e63fee23c10d",
   "metadata": {},
   "source": [
    "### Forward Pass\n",
    "\n",
    "The forward pass of the logistic regression model is given by:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\sigma(w_1x_1 + w_2x_2 + b)\n",
    "$$\n",
    "\n",
    "where $\\sigma$ is the sigmoid function, $w_1$ and $w_2$ are the weights, $x_1$ and $x_2$ are the inputs, and $b$ is the bias.\n",
    "\n",
    "The sigmoid function is given by:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "We will implement the forward pass as a function that takes the inputs, weights, and bias as arguments and returns the predicted output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a990a579-7666-4a6c-8bd7-8d69e8b5c0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    #TODO: Implement the sigmoid function\n",
    "    pass\n",
    "\n",
    "def forward_pass(x1, x2, w1, w2, b):\n",
    "    #TODO: Implement the forward pass function\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67db39d-6d75-40b8-a0e6-ddd0b22684cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the forward pass function\n",
    "y_pred = forward_pass(0, 0, 1, 1, 0.5)\n",
    "assert math.isclose(y_pred, 0.6224593312018546, rel_tol=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7562231a-f2b0-4e0c-b704-e32c2d5d4d7b",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "\n",
    "The loss function of the logistic regression model is given by the binary cross-entropy loss function:\n",
    "\n",
    "$$\n",
    "L(y, \\hat{y}) = -y \\log(\\hat{y}) - (1 - y) \\log(1 - \\hat{y})\n",
    "$$\n",
    "\n",
    "where $y$ is the true label and $\\hat{y}$ is the predicted output.\n",
    "\n",
    "We will implement the loss function as a function that takes the true label and predicted output as arguments and returns the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c8af81-7dc1-4cae-adab-43297068285a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y, y_pred):\n",
    "    # TODO: Implement the loss function\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4662e30-f69c-43ae-9f05-b7ba14417229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the loss function\n",
    "y_true = 0\n",
    "y_pred = 0.6224593312018546\n",
    "loss_val = loss(y_true, y_pred)\n",
    "assert math.isclose(loss_val, 0.9740769841801068, rel_tol=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959d2cee-587c-447f-bf0e-19f09e85d353",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "The gradient descent algorithm is used to update the weights and bias of the logistic regression model in order to minimize the loss function. The update rule for the weights and bias is given by:\n",
    "\n",
    "$$\n",
    "w_i = w_i - \\alpha \\frac{\\partial L}{\\partial w_i}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b = b - \\alpha \\frac{\\partial L}{\\partial b}\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is the learning rate.\n",
    "\n",
    "The partial derivatives of the loss function with respect to the weights and bias are given by:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_1} = x_1(\\hat{y} - y)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_2} = x_2(\\hat{y} - y)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b} = \\hat{y} - y\n",
    "$$\n",
    "\n",
    "We will implement the gradient computation and parameter update functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912991d5-91b9-4557-8cb4-adcab2b5976f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(x, y_true, y_pred):\n",
    "    # TODO: Implement the gradient computation function\n",
    "    return dw1, dw2, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbda0287-c189-4f54-b578-aed6b30a21ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the gradient computation function\n",
    "x = [0, 0]\n",
    "y_true = 0\n",
    "y_pred = 0.6224593312018546\n",
    "dw1, dw2, db = compute_gradients(x, y_true, y_pred)\n",
    "assert math.isclose(dw1, 0, rel_tol=1e-9)\n",
    "assert math.isclose(dw2, 0, rel_tol=1e-9)\n",
    "assert math.isclose(db, 0.6224593312018546, rel_tol=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486b367c-e98e-4422-89ce-a503d0c6d811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(w1, w2, b, dw1, dw2, db, learning_rate):\n",
    "    #TODO: Implement the parameter update function\n",
    "    return w1, w2, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6c8c57-2017-4a95-92af-158416d39d74",
   "metadata": {},
   "source": [
    "### Create the Model\n",
    "\n",
    "We will now create the logistic regression model using the gradient descent algorithm. We will initialize the weights and bias, set the learning rate, and train the model for a fixed number of epochs.\n",
    "\n",
    "The logistic regression class will keep track of losses and accuracies during training. We will implement the training loop and evaluate the model on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc12840-2125-4beb-ac14-9ac1f9761805",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self):\n",
    "        self.w1 = np.random.randn()\n",
    "        self.w2 = np.random.randn()\n",
    "        self.b = np.random.randn()\n",
    "        self.losses = []\n",
    "        self.accuracies = []\n",
    "\n",
    "    def train(self, X, y, learning_rate, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            correct = 0\n",
    "            for i in range(len(X)):\n",
    "                x = X[i]\n",
    "                y_true = y[i]\n",
    "                y_pred = forward_pass(x[0], x[1], self.w1, self.w2, self.b)\n",
    "                epoch_loss += loss(y_true, y_pred)\n",
    "                dw1, dw2, db = compute_gradients(x, y_true, y_pred)\n",
    "                self.w1, self.w2, self.b = update_parameters(self.w1, self.w2, self.b, dw1, dw2, db, learning_rate)\n",
    "                if y_pred >= 0.5 and y_true == 1 or y_pred < 0.5 and y_true == 0:\n",
    "                    correct += 1\n",
    "            accuracy = correct / len(X)\n",
    "            self.losses.append(epoch_loss)\n",
    "            self.accuracies.append(accuracy)\n",
    "            print(f\"Epoch {epoch + 1}: Loss = {epoch_loss}, Accuracy = {accuracy}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            y_pred = forward_pass(x[0], x[1], self.w1, self.w2, self.b)\n",
    "            predictions.append(y_pred)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb29e87a-130e-4010-a46d-bd09491e176e",
   "metadata": {},
   "source": [
    "### Train the Model\n",
    "\n",
    "We will now create an instance of the logistic regression model, train it on the NAND gate data, and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca85cff-997c-4fed-92c2-8c44802a7efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([1, 1, 1, 0])\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.train(X, y, learning_rate=0.1, epochs=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f61c385-4f8f-46c8-9362-5ba5d8dacce2",
   "metadata": {},
   "source": [
    "### Evaluate the Model\n",
    "\n",
    "We will now evaluate the model on the training data and plot the loss and accuracy curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a530dc2-d427-4d2c-ac52-572d9d54c042",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model.losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb073093-4fe4-4581-8647-2d1106f1870c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model.accuracies)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f1aab3-afe8-47ae-9871-1e3fe45cdcb2",
   "metadata": {},
   "source": [
    "### Predictions\n",
    "\n",
    "We will now make predictions using the trained model and compare them with the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b63d67-5940-4c42-98e6-903f64217bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X)\n",
    "for i in range(len(X)):\n",
    "    print(f\"Input: {X[i]}, Predicted Output: {predictions[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67725aa-4a82-49c5-8dac-9f6a12fb4ae8",
   "metadata": {},
   "source": [
    "### Plot the Decision Boundary\n",
    "\n",
    "We will now plot the decision boundary of the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d20a823-be39-4fff-bcbd-3fc3a017ae28",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.linspace(-0.5, 1.5, 100)\n",
    "x2 = np.linspace(-0.5, 1.5, 100)\n",
    "X1, X2 = np.meshgrid(x1, x2)\n",
    "Z = forward_pass(X1, X2, model.w1, model.w2, model.b)\n",
    "\n",
    "plt.contourf(X1, X2, Z, levels=1, colors=['blue', 'red'], alpha=0.3)\n",
    "plt.scatter(x=[1],y=[1])\n",
    "plt.scatter(x=[0,0,1],y=[0,1,0])\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.title('Decision Boundary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66c3470",
   "metadata": {},
   "source": [
    "Credits: Pattern Recognition TAs"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
